{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7f6567",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b747b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "api_key = \"\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image to a base64 string.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path of the image to encode.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64-encoded string of the image, or None if an error occurs.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return None\n",
    "    \n",
    "def llm_chat_completion(messages, model=\"gpt-4o-mini\", max_tokens=300, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Calls OpenAI's ChatCompletion API with the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message dictionaries for the conversation.\n",
    "        model (str): The model to use for the chat completion.\n",
    "        max_tokens (int, optional): Maximum tokens for the response. Defaults to 300.\n",
    "        temperature (float, optional): Sampling temperature for randomness. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The response content from the API, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response =  client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_few_shot_messages(few_shot_prompt, user_prompt = \"Please analyze the following image:\", detail=\"auto\"):\n",
    "    \"\"\"\n",
    "    Generates few-shot example messages from image-caption pairs.\n",
    "\n",
    "    Args:\n",
    "        few_shot_prompt (dict): A dictionary mapping image paths to metadata, \n",
    "                                including \"image_caption\".\n",
    "        detail (str, optional): Level of image detail to include. Defaults to \"auto\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of few-shot example messages.\n",
    "    \"\"\"\n",
    "    few_shot_messages = []\n",
    "    for path, data in few_shot_prompt.items():\n",
    "        base64_image = encode_image(path)\n",
    "        if not base64_image:\n",
    "            continue  # skip if failed to encode\n",
    "        caption = data\n",
    "        \n",
    "        few_shot_messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": detail\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        few_shot_messages.append({\"role\": \"assistant\", \"content\": caption})\n",
    "    return few_shot_messages\n",
    "\n",
    "def build_user_message(image_path, user_prompt=\"Please analyze the following image:\", detail=\"auto\"):\n",
    "    \"\"\"\n",
    "    Creates a user message for analyzing a single image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        detail (str, optional): Level of image detail to include. Defaults to \"auto\".\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The user message dictionary, or None if image encoding fails.\n",
    "    \"\"\"\n",
    "    base64_image = encode_image(image_path)\n",
    "    if not base64_image:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\": detail\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def get_image_caption(\n",
    "    image_path,\n",
    "    few_shot_prompt=None,\n",
    "    system_prompt=\"You are a helpful assistant that can analyze images and provide captions.\",\n",
    "    user_prompt=\"Please analyze the following image:\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=300,\n",
    "    detail=\"auto\",\n",
    "    llm_chat_func=llm_chat_completion,\n",
    "    temperature=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets a caption for an image using a LLM.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): File path of the image to be analyzed.\n",
    "        few_shot_prompt (dict, optional): Maps image paths to {\"image_caption\": <caption>}.\n",
    "        system_prompt (str, optional): Initial system prompt for the LLM.\n",
    "        user_prompt (str, optional): User prompt for the LLM.\n",
    "        model (str, optional): LLM model name (default \"gpt-4o-mini\").\n",
    "        max_tokens (int, optional): Max tokens in the response (default 300).\n",
    "        detail (str, optional): Level of detail for the image analysis (default \"auto\").\n",
    "        llm_chat_func (callable, optional): Function to call the LLM. Defaults to `llm_chat_completion`.\n",
    "        temperature (float, optional): Sampling temperature (default 0.0).\n",
    "\n",
    "    Returns:\n",
    "        str or None: The generated caption, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_message = build_user_message(image_path, detail)\n",
    "        if not user_message:\n",
    "            return None\n",
    "        \n",
    "        # Build message sequence\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "        # Include few-shot examples if provided\n",
    "        if few_shot_prompt:\n",
    "            few_shot_messages = build_few_shot_messages(few_shot_prompt, detail)\n",
    "            messages.extend(few_shot_messages)\n",
    "\n",
    "        messages.append(user_message)\n",
    "\n",
    "        # Call the LLM\n",
    "        response_text = llm_chat_func(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting caption: {e}\")\n",
    "        return None\n",
    "    \n",
    "def process_images_in_parallel(\n",
    "    image_paths, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    system_prompt=\"You are a helpful assistant that can analyze images and provide captions.\", \n",
    "    user_prompt=\"Please analyze the following image:\", \n",
    "    few_shot_prompt = None, \n",
    "    max_tokens=300, \n",
    "    detail=\"auto\", \n",
    "    max_workers=5):\n",
    "    \"\"\"\n",
    "    Processes a list of images in parallel to generate captions using a specified model.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of file paths to the images to be processed.\n",
    "        model (str): The model to use for generating captions (default is \"gpt-4o\").\n",
    "        max_tokens (int): Maximum number of tokens in the generated captions (default is 300).\n",
    "        detail (str): Level of detail for the image analysis (default is \"auto\").\n",
    "        max_workers (int): Number of threads to use for parallel processing (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are image paths and values are their corresponding captions.\n",
    "    \"\"\"    \n",
    "    captions = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Pass additional arguments using a lambda or partial\n",
    "        future_to_image = {\n",
    "            executor.submit(\n",
    "                get_image_caption, \n",
    "                image_path, \n",
    "                few_shot_prompt, \n",
    "                system_prompt, \n",
    "                user_prompt, \n",
    "                model, \n",
    "                max_tokens, \n",
    "                detail): image_path\n",
    "            for image_path in image_paths\n",
    "        }\n",
    "\n",
    "        # Use tqdm to track progress\n",
    "        for future in tqdm(as_completed(future_to_image), total=len(image_paths), desc=\"Processing images\"):\n",
    "            image_path = future_to_image[future]\n",
    "            try:\n",
    "                caption = future.result()\n",
    "                captions[image_path] = caption\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "                captions[image_path] = None\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659a738",
   "metadata": {},
   "source": [
    "## Process Zero-Shot Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_QUALITY = \"high\"\n",
    "PATH_TO_SAMPLES = \"images/\"\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant that provides captions of images. \n",
    "You will be provided with an image. Analyze the content, context, and notable features of the images.\n",
    "Provide a concise caption that covers the important aspects of the image.\"\"\"\n",
    "\n",
    "user_prompt = \"Please analyze the following image:\"\n",
    "\n",
    "image_paths = [os.path.join(PATH_TO_SAMPLES, x) for x in os.listdir(PATH_TO_SAMPLES)]\n",
    "\n",
    "zero_shot_high_quality_captions = process_images_in_parallel(image_paths, model = \"gpt-4o-mini\", system_prompt=system_prompt, user_prompt = user_prompt, few_shot_prompt= None, detail=IMAGE_QUALITY, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82add0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/zero_shot_captions_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(zero_shot_high_quality_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32c5ee",
   "metadata": {},
   "source": [
    "## Perform few-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captions = json.load(open(\"image_captions.json\"))\n",
    "\n",
    "FEW_SHOT_EXAMPLES_PATH = \"few_shot_examples/\"\n",
    "\n",
    "few_shot_samples = os.listdir(FEW_SHOT_EXAMPLES_PATH)\n",
    "few_shot_captions = {os.path.join(FEW_SHOT_EXAMPLES_PATH,k):v for k,v in  image_captions.items() if k in few_shot_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac42c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_QUALITY = \"high\"\n",
    "few_shot_high_quality_captions = process_images_in_parallel(image_paths, model = \"gpt-4o-mini\", few_shot_prompt= few_shot_captions, detail=IMAGE_QUALITY, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7941e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/few_shot_captions_results.json\", \"w\") as f:\n",
    "    f.write(json.dumps(few_shot_high_quality_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b45504",
   "metadata": {},
   "source": [
    "## Perform Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80714e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_samples_1_cot = \"\"\"Observations:\n",
    "Visible Blur: The foreground and parts of the image are out of focus or blurred, indicating either camera movement or subject motion during the shot.\n",
    "Tall, Ornate Buildings: The structures have multiple floors, detailed balconies, and decorative facades, suggesting older or classic urban architecture.\n",
    "Street-Level View: Parked cars line both sides of a road or narrow street, confirming an urban environment with typical city traffic and infrastructure.\n",
    "Soft, Warm Light: The sunlight appears to be hitting the buildings at an angle, creating a warm glow on the façade and enhancing the sense of a real city scene rather than a staged setup.\n",
    "\n",
    "Final Caption: A blurry photo of a city street with buildings.\"\"\"\n",
    "\n",
    "few_shot_samples_2_cot = \"\"\"Observations:  \n",
    "Elevated Desert Dunes: The landscape is made up of large, rolling sand dunes in a dry, arid environment.  \n",
    "Off-Road Vehicle: The white SUV appears equipped for travel across uneven terrain, indicated by its size and ground clearance.  \n",
    "Tire Tracks in the Sand: Visible tracks show recent movement across the dunes, reinforcing that the vehicle is in motion and navigating a desert path.  \n",
    "View from Inside Another Car: The dashboard and windshield framing in the foreground suggest the photo is taken from a passenger’s or driver’s perspective, following behind or alongside the SUV.  \n",
    "\n",
    "Final Caption: A white car driving on a desert road.\"\"\"\n",
    "\n",
    "\n",
    "few_shot_samples_3_cot = \"\"\"Observations:\n",
    "Towering Rock Formations: The steep canyon walls suggest a rugged desert landscape, with sandstone cliffs rising on both sides.\n",
    "Illuminated Tents: Two futuristic-looking tents emit a soft glow, indicating a nighttime scene with lights or lanterns inside.\n",
    "Starry Night Sky: The visible stars overhead reinforce that this is an outdoor camping scenario after dark.\n",
    "Single Male Figure: A man, seen from behind, stands near one of the tents, indicating he is likely part of the camping group.\n",
    "\n",
    "Final Caption: A man standing next to a tent in the desert.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "few_shot_samples_cot = copy.deepcopy(few_shot_captions)\n",
    "\n",
    "few_shot_samples_cot[\"few_shot_examples/photo_1.jpg\"] = few_shot_samples_1_cot\n",
    "few_shot_samples_cot[\"few_shot_examples/photo_3.jpg\"] = few_shot_samples_2_cot\n",
    "few_shot_samples_cot[\"few_shot_examples/photo_4.jpg\"] = few_shot_samples_3_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_QUALITY = \"high\"\n",
    "\n",
    "system_prompt_cot_reasoning = \"\"\"You are an AI assistant that generates captions for images. \n",
    "You will be provided with an image. Your task is to carefully analyze the content, context, and notable features of the image. \n",
    "Break down your reasoning into clear, intermediate observations, considering both prominent and subtle details. \n",
    "Finally, create a concise caption that accurately summarizes the key elements and context of the image.\"\"\"\n",
    "\n",
    "user_prompt = \"Please analyze the following image:\"\n",
    "\n",
    "cot_high_quality_captions = process_images_in_parallel(image_paths, model = \"gpt-4o-mini\", system_prompt=system_prompt_cot_reasoning, user_prompt=user_prompt, few_shot_prompt= few_shot_samples_cot, detail=IMAGE_QUALITY, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ae300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_final_caption(text):\n",
    "    \"\"\"\n",
    "    Extracts the content that comes after \"Final Caption\" in the provided text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text from which to extract the caption.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted caption or an empty string if no caption is found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pattern = r\"(?<=Final Caption: ).*\"\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group()\n",
    "    except:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_high_quality_captions_extracted = {}\n",
    "captions = []\n",
    "\n",
    "for k, v in cot_high_quality_captions.items():\n",
    "    captions_extracted = extract_final_caption(v)\n",
    "    cot_high_quality_captions_extracted[k] = captions_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in cot_high_quality_captions.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/cot_high_quality_captions_results.json\", \"w\") as f:\n",
    "    f.write(json.dumps(cot_high_quality_captions_extracted))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7f4a5",
   "metadata": {},
   "source": [
    "## Utilize Object Detection Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_QUALITY = \"high\"\n",
    "system_prompt_object_detection = \"\"\"You are provided with an image. You must identify all important objects in the image, and provide a standardized list of objects in the image.\n",
    "Return your output as follows:\n",
    "Output: object_1, object_2\"\"\"\n",
    "\n",
    "user_prompt = \"Extract the objects from the provided image:\"\n",
    "\n",
    "detected_objects = process_images_in_parallel(image_paths, system_prompt=system_prompt_object_detection, user_prompt=user_prompt, model = \"gpt-4o-mini\", few_shot_prompt= None, detail=IMAGE_QUALITY, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_objects_cleaned = {}\n",
    "\n",
    "for key, value in detected_objects.items():\n",
    "    detected_objects_cleaned[key] = list(set([x.strip() for x in value.replace(\"Output: \", \"\").split(\",\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faed965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def detect_and_draw_bounding_boxes(\n",
    "    image_path,\n",
    "    text_queries,\n",
    "    model,\n",
    "    processor,\n",
    "    output_path,\n",
    "    score_threshold=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect objects in an image and draw bounding boxes over the original image using PIL.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "    - text_queries (list of str): List of text queries to process.\n",
    "    - model: Pretrained model to use for detection.\n",
    "    - processor: Processor to preprocess image and text queries.\n",
    "    - output_path (str): Path to save the output image with bounding boxes.\n",
    "    - score_threshold (float): Threshold to filter out low-confidence predictions.\n",
    "\n",
    "    Returns:\n",
    "    - output_image_pil: A PIL Image object with bounding boxes and labels drawn.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    orig_w, orig_h = img.size  # original width, height\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text_queries,\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = torch.max(outputs[\"logits\"][0], dim=-1)       # shape (num_boxes,)\n",
    "    scores = torch.sigmoid(logits.values).cpu().numpy()    # convert to probabilities\n",
    "    labels = logits.indices.cpu().numpy()                  # class indices\n",
    "    boxes_norm = outputs[\"pred_boxes\"][0].cpu().numpy()    # shape (num_boxes, 4)\n",
    "\n",
    "    converted_boxes = []\n",
    "    for box in boxes_norm:\n",
    "        cx, cy, w, h = box\n",
    "        cx_abs = cx * orig_w\n",
    "        cy_abs = cy * orig_h\n",
    "        w_abs  = w  * orig_w\n",
    "        h_abs  = h  * orig_h\n",
    "        x1 = cx_abs - w_abs / 2.0\n",
    "        y1 = cy_abs - h_abs / 2.0\n",
    "        x2 = cx_abs + w_abs / 2.0\n",
    "        y2 = cy_abs + h_abs / 2.0\n",
    "        converted_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    for score, (x1, y1, x2, y2), label_idx in zip(scores, converted_boxes, labels):\n",
    "        if score < score_threshold:\n",
    "            continue\n",
    "\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "\n",
    "        label_text = text_queries[label_idx].replace(\"An image of \", \"\")\n",
    "\n",
    "        text_str = f\"{label_text}: {score:.2f}\"\n",
    "        text_size = draw.textsize(text_str)  # If no font used, remove \"font=font\"\n",
    "        text_x, text_y = x1, max(0, y1 - text_size[1])  # place text slightly above box\n",
    "\n",
    "        draw.rectangle(\n",
    "            [text_x, text_y, text_x + text_size[0], text_y + text_size[1]],\n",
    "            fill=\"white\"\n",
    "        )\n",
    "        draw.text((text_x, text_y), text_str, fill=\"red\")  # , font=font)\n",
    "\n",
    "    img.save(output_path, \"JPEG\")\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/detected_objects.json\", \"w\") as f:\n",
    "    f.write(json.dumps(detected_objects_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in tqdm(detected_objects_cleaned.items()):\n",
    "    value = [\"An image of \" + x for x in value]\n",
    "    detect_and_draw_bounding_boxes(key, value, model, processor, \"images_with_bounding_boxes/\" + key.split(\"/\")[-1], score_threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff401b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_QUALITY = \"high\"\n",
    "image_paths_obj_detected_guided = [x.replace(\"downloaded_images\", \"images_with_bounding_boxes\") for x in image_paths] \n",
    "\n",
    "system_prompt_obj_det=\"\"\"You are a helpful assistant that can analyze images and provide captions. You are provided with images that also contain bounding box annotations of the important objects in them, along with their labels.\n",
    "Analyze the overall image and the provided bounding box information and provide an appropriate caption for the image.\"\"\",\n",
    "\n",
    "user_prompt=\"Please analyze the following image:\",\n",
    "\n",
    "obj_det_zero_shot_high_quality_captions = process_images_in_parallel(image_paths_obj_detected_guided, system_prompt=system_prompt_obj_det, user_prompt=user_prompt, model = \"gpt-4o-mini\", few_shot_prompt= None, detail=IMAGE_QUALITY, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/zero_shot_object_detection_guided_captions.json\", \"w\") as f:\n",
    "    f.write(json.dumps(obj_det_zero_shot_high_quality_captions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
