{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff1289-7310-4d18-99bb-cddda849ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "random.seed(42)\n",
    "from helpers import *\n",
    "from retriever import *\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78bb06-5dd8-4b6b-8115-a89f7371a183",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34fae9e-2411-4fc2-bcbd-277d1a2c6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",  torch_dtype=torch.bfloat16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896f301-89af-4329-b6d8-191f460b5f9f",
   "metadata": {},
   "source": [
    "## Create test set subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a8b8e-083f-4c3a-845d-d6b2812a5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = parse_dataset(\"CDR_TrainingSet.PubTator.txt\")\n",
    "# deduplicate_annotations(train_dataset)\n",
    "\n",
    "# test_dataset = parse_dataset(\"CDR_TestSet.PubTator.txt\")\n",
    "# deduplicate_annotations(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25224d-05ed-4084-8c20-3b8b2b0e130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# few_shot_prompt = [train_dataset[0] , train_dataset[10] , train_dataset[100]]\n",
    "# test_set_subsample = random.sample(test_dataset, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeae69-8a63-4ba6-97e6-195fcbd59f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_jsonl_file(\"mesh_few_shot_prompt.jsonl\",few_shot_prompt)\n",
    "# write_jsonl_file(\"test_set_subsample.jsonl\",test_set_subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c06411",
   "metadata": {},
   "source": [
    "## Load test set subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa897c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_subsample = read_jsonl_file(\"test_set_subsample.jsonl\")\n",
    "few_shot_example = read_jsonl_file(\"mesh_few_shot_prompt.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f033938-5c11-4d92-af9a-544d0ae48934",
   "metadata": {},
   "source": [
    "# Evaluate zero-shot Mistral performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4732f8b-462d-4b31-a6b9-ec0b248a60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_few_shot_answers = []\n",
    "for item in tqdm(test_set_subsample):\n",
    "    few_shot_prompt_messages = build_few_shot_prompt(SYSTEM_PROMPT, item, few_shot_example)\n",
    "    input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors = \"pt\").cuda()\n",
    "    outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    \n",
    "    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554\n",
    "    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    mistral_few_shot_answers.append(parse_answer(gen_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038c03e-101d-4359-9a02-a83905c70618",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mistral_zero_shot_predictions.json\", \"w\") as file:\n",
    "    file.write(json.dumps({\"predictions\": mistral_few_shot_answers}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093d07d-f739-4af2-a5a2-1b6a9dd7d789",
   "metadata": {},
   "source": [
    "## Evaluate zero-shot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec0631-4914-4638-b3a0-329407567385",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_scores = [calculate_entity_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, mistral_few_shot_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c427ba5-50ed-4b0a-9aa0-0125e3b3c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)\n",
    "macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)\n",
    "macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ac124-cbbc-41c2-a028-40e6f6fa859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, mistral_few_shot_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b11dd9-31fe-4b14-a9bc-4df65399b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)\n",
    "macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)\n",
    "macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50c057-5179-448b-994b-8bec856ba2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame([[\"Entity Extraction\", macro_precision_entity, macro_recall_entity, macro_f1_entity], [\"Entity Linking\", macro_precision_mesh, macro_recall_mesh,macro_f1_mesh]])\n",
    "scores_df.columns = [\"Task\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"]\n",
    "scores_df['Macro Precision'] = scores_df['Macro Precision'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df['Macro Recall'] = scores_df['Macro Recall'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df['Macro F1'] = scores_df['Macro F1'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df.to_csv(\"zero_shot_entity_mesh_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9ecea",
   "metadata": {},
   "source": [
    "## Set up BM-25 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_data = read_jsonl_file(\"mesh_2020.jsonl\")\n",
    "process_mesh_kb(mesh_data)\n",
    "mesh_data_kb = {x[\"concept_id\"]:x for x in mesh_data}\n",
    "mesh_data_dict = process_index({x[\"concept_id\"]:x for x in mesh_data})\n",
    "entity_mesh_data_dict = [[x[\"concept_id\"] , \" \".join(x[\"aliases\"].split(\",\")) + \" \" + x[\"canonical_name\"]] for x in mesh_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6706ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(mesh_data_dict)\n",
    "entity_retriever = BM25Retriever(entity_mesh_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641f999-9304-442f-9743-ac9ed6cbcec4",
   "metadata": {},
   "source": [
    "### Evaluate Zero-Shot + Retriever Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f91d31-6dce-48ca-a541-add875fb940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_entities_few_shot = [[y[\"text\"] for y in x] for x in mistral_few_shot_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060b237-a14e-495e-be32-22a2155f5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_answers = []\n",
    "\n",
    "for item in tqdm(parsed_entities_few_shot):\n",
    "    answer_element = []\n",
    "    for entity in item:\n",
    "        retrieved_mesh_ids = entity_retriever.query(entity, top_n = 1)\n",
    "        answer_element.append({\"text\": entity, \"identifier\":retrieved_mesh_ids[0]})\n",
    "    retrieved_answers.append(answer_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea9f6b-2685-43bb-bfd0-33bc30b5813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mistral_zero_shot_predictions_external_retriever.json\", \"w\") as file:\n",
    "    file.write(json.dumps({\"predictions\": retrieved_answers}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa88f68-71af-44f5-8ac6-da8bfbf8cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa45bd0-444b-426d-bce9-7016202d719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(entity_scores)\n",
    "macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(entity_scores)\n",
    "macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(entity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47300356-63fb-4d6d-a755-611db708f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame([[\"Entity Extraction\", macro_precision_entity, macro_recall_entity, macro_f1_entity], [\"Entity Linking\", macro_precision_mesh, macro_recall_mesh,macro_f1_mesh]])\n",
    "scores_df.columns = [\"Task\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"]\n",
    "scores_df['Macro Precision'] = scores_df['Macro Precision'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df['Macro Recall'] = scores_df['Macro Recall'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df['Macro F1'] = scores_df['Macro F1'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "scores_df.to_csv(\"zero_shot_entity_external_retriever_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021faab4-f636-45ba-b8ea-0bbae2f9b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mistral_zero_shot_entity_retriever_predictions.json\", \"w\") as file:\n",
    "    file.write(json.dumps({\"predictions\": retrieved_answers}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129e8c9",
   "metadata": {},
   "source": [
    "## Evaluate RAG performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646971fc-658c-42e1-a815-33690ed9ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_dict = {}\n",
    "ground_truth_ids = []\n",
    "retrieved_ids = []\n",
    "\n",
    "for item in tqdm(test_set_subsample):\n",
    "    relevant_mesh_ids = retriever.query(item[\"title\"] + \" \" + item[\"abstract\"], top_n = 50)\n",
    "    gt_ids = [x[\"identifier\"] for x in item[\"annotations\"]]\n",
    "    ground_truth_ids.append(gt_ids)\n",
    "    retrieved_ids.append(relevant_mesh_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3b89a-1398-4f60-bcbb-ee614ab33d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_coverage_dict = {10:[], 30:[], 50:[]}\n",
    "for gt, pred in zip(ground_truth_ids, retrieved_ids):\n",
    "    for k in [10,30,50]:\n",
    "        reqd_pred = pred[0:k]\n",
    "        percent_gt_in_retrieved = set(gt).intersection(set(reqd_pred))\n",
    "        percent_coverage_dict[k].append(len(percent_gt_in_retrieved) / len(gt))\n",
    "\n",
    "for key in percent_coverage_dict:\n",
    "    percent_coverage_dict[key] = (sum(percent_coverage_dict[key]) / len(percent_coverage_dict[key])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f89c9-88df-42fd-b990-aef763f922df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(percent_coverage_dict.keys(), percent_coverage_dict.values(), marker='o', linestyle='-', color='b')\n",
    "plt.title('On average, what % of Ground Truth IDs is present in the fetched results?')\n",
    "plt.xlabel('No. of Retrieved IDs')\n",
    "plt.ylabel('Avg proportion of retrieved GT IDs')\n",
    "plt.grid(True)\n",
    "plt.xticks(list(percent_coverage_dict.keys()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249e6f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mistral_rag_answers = {10:[], 30:[], 50:[]}\n",
    "\n",
    "for k in [10,30,50]:\n",
    "    for item in tqdm(test_set_subsample):\n",
    "        relevant_mesh_ids = retriever.query(item[\"title\"] + \" \" + item[\"abstract\"], top_n = k)\n",
    "        relevant_contexts = [mesh_data_kb[x] for x in relevant_mesh_ids]\n",
    "        rag_prompt = build_rag_prompt(SYSTEM_RAG_PROMPT, item, relevant_contexts)\n",
    "        input_ids = tokenizer.apply_chat_template(rag_prompt, tokenize=True, return_tensors = \"pt\").cuda()\n",
    "        outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    \n",
    "        # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554\n",
    "        gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "        mistral_rag_answers[k].append(parse_answer(gen_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098fc06-d2f3-40f5-af07-0ec705e19cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_scores_at_k = {}\n",
    "mesh_scores_at_k = {}\n",
    "df_list = []\n",
    "\n",
    "for key, value in mistral_rag_answers.items():\n",
    "    entity_scores = [calculate_entity_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, value)]\n",
    "    macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)\n",
    "    macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)\n",
    "    macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)\n",
    "    entity_scores_at_k[key] = {\"macro-precision\": macro_precision_entity, \"macro-recall\": macro_recall_entity, \"macro-f1\": macro_f1_entity}\n",
    "    \n",
    "    mesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, value)]\n",
    "    macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)\n",
    "    macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)\n",
    "    macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)\n",
    "    mesh_scores_at_k[key] = {\"macro-precision\": macro_precision_mesh, \"macro-recall\": macro_recall_mesh, \"macro-f1\": macro_f1_mesh}\n",
    "\n",
    "    scores_df = pd.DataFrame([[\"Entity Extraction\", macro_precision_entity, macro_recall_entity, macro_f1_entity], [\"Entity Linking\", macro_precision_mesh, macro_recall_mesh,macro_f1_mesh]])\n",
    "    scores_df.columns = [\"Task\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"]\n",
    "    scores_df['Macro Precision'] = scores_df['Macro Precision'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "    scores_df['Macro Recall'] = scores_df['Macro Recall'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "    scores_df['Macro F1'] = scores_df['Macro F1'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "    df_list.append(scores_df)\n",
    "    \n",
    "writer = pd.ExcelWriter('results_rag.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each DataFrame to a different worksheet\n",
    "df_list[0].to_excel(writer, sheet_name='Rag@10', index=False)\n",
    "df_list[1].to_excel(writer, sheet_name='Rag@30', index=False)\n",
    "df_list[2].to_excel(writer, sheet_name='Rag@50', index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7f24e-3c24-4850-a8e1-1d025555907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mistral_rag_predictions.json\", \"w\") as file:\n",
    "    file.write(json.dumps({\"predictions\": mistral_rag_answers}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593d834-1c76-4aab-96ea-3887f0e21681",
   "metadata": {},
   "source": [
    "## Plot the entity scores as a function of number of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6163ff-93c9-476a-b81c-17c09e4597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = list(entity_scores_at_k.keys())\n",
    "y_precision = [details['macro-precision'] * 100 for details in entity_scores_at_k.values()]\n",
    "y_recall = [details['macro-recall'] * 100 for details in entity_scores_at_k.values()]\n",
    "y_f1 = [details['macro-f1'] * 100 for details in entity_scores_at_k.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y_precision, marker='o', linestyle='-', color='r', label='Macro-Precision')\n",
    "plt.plot(x, y_recall, marker='^', linestyle='-', color='g', label='Macro-Recall')\n",
    "plt.plot(x, y_f1, marker='s', linestyle='-', color='b', label='Macro-F1')\n",
    "\n",
    "plt.title('Entity Extraction Performance')\n",
    "plt.xlabel('No. of Retrieved IDs')\n",
    "plt.ylabel('Scores (%)')\n",
    "plt.grid(True)\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175fd5ed-c0de-451f-914b-096661e4c63e",
   "metadata": {},
   "source": [
    "## Plot the MeSH linking scores as a function of number of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eeccac-b7b3-4c52-b1fa-3dbc0dbcb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(mesh_scores_at_k.keys())\n",
    "y_precision = [details['macro-precision'] * 100 for details in mesh_scores_at_k.values()]\n",
    "y_recall = [details['macro-recall'] * 100 for details in mesh_scores_at_k.values()]\n",
    "y_f1 = [details['macro-f1'] * 100 for details in mesh_scores_at_k.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y_precision, marker='o', linestyle='-', color='r', label='Macro-Precision')\n",
    "plt.plot(x, y_recall, marker='^', linestyle='-', color='g', label='Macro-Recall')\n",
    "plt.plot(x, y_f1, marker='s', linestyle='-', color='b', label='Macro-F1')\n",
    "\n",
    "plt.title('Entity Linking Performance')\n",
    "plt.xlabel('No. of Retrieved IDs')\n",
    "plt.ylabel('Scores (%)')\n",
    "plt.grid(True)\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
